{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SCALA_VERSION=\"2.12\"\n",
    "SPARK_VERSION=\"3.0.1\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f\"--packages=org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell\"\n",
    "\n",
    "import json\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Version expected 3.8.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.6\r\n"
     ]
    }
   ],
   "source": [
    "# Python 3.8.6\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confluent-kafka version 1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: confluent-kafka\r\n",
      "Version: 1.5.0\r\n",
      "Summary: Confluent's Python client for Apache Kafka\r\n",
      "Home-page: https://github.com/confluentinc/confluent-kafka-python\r\n",
      "Author: Confluent Inc\r\n",
      "Author-email: support@confluent.io\r\n",
      "License: UNKNOWN\r\n",
      "Location: /opt/conda/lib/python3.8/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "# Version: 1.5.0\n",
    "!pip show confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('RealtimeKafkaML') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.0.1\n"
     ]
    }
   ],
   "source": [
    "# Spark 3.0.1\n",
    "print(f\"Spark {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_messages = [\n",
    "'I love this pony',\n",
    "'This restaurant is great',\n",
    "'The weather is bad today',\n",
    "'I will go to the beach this weekend',\n",
    "'She likes to swim',\n",
    "'Apple is a great company'\n",
    "]\n",
    "\n",
    "bootstrap_servers = 'kafka:9092'\n",
    "topic = 'test'\n",
    "msg_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_report(err, msg):    \n",
    "    \"\"\" Called once for each message produced to indicate delivery result.\n",
    "        Triggered by poll() or flush(). \"\"\"\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {}'.format(msg.topic()))\n",
    "\n",
    "def confluent_kafka_producer():\n",
    "    \n",
    "    p = Producer({'bootstrap.servers': bootstrap_servers})\n",
    "    for data in simple_messages:\n",
    "        \n",
    "        record_key = str(uuid.uuid4())\n",
    "        record_value = json.dumps({'data': data})\n",
    "        p.produce(topic, key=record_key, value=record_value, on_delivery=delivery_report)\n",
    "       \n",
    "        p.poll(0)\n",
    "\n",
    "    p.flush()\n",
    "    print('we\\'ve sent {count} messages to {brokers}'.format(count=len(simple_messages), brokers=bootstrap_servers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message delivered to test\n",
      "Message delivered to test\n",
      "Message delivered to test\n",
      "Message delivered to test\n",
      "Message delivered to test\n",
      "Message delivered to test\n",
      "we've sent 6 messages to kafka:9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-294006cfd851>:16: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(topic, key=record_key, value=record_value, on_delivery=delivery_report)\n"
     ]
    }
   ],
   "source": [
    "confluent_kafka_producer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read From Kafka to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark \\\n",
    "  .readStream \\\n",
    "  .format('kafka') \\\n",
    "  .option('kafka.bootstrap.servers', bootstrap_servers) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option('subscribe', topic) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df_raw.selectExpr('CAST(value AS STRING) as json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_json \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"df_json\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Available: False'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                json|\n",
      "+--------------------+\n",
      "|{\"data\": \"She lik...|\n",
      "|{\"data\": \"Apple i...|\n",
      "|{\"data\": \"I will ...|\n",
      "|{\"data\": \"I love ...|\n",
      "|{\"data\": \"The wea...|\n",
      "|{\"data\": \"This re...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"Data Available: {query.status['isDataAvailable']}\")\n",
    "display(spark.sql('SELECT DISTINCT json FROM df_json').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f27fbb78640>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([StructField('data', StringType())])\n",
    "\n",
    "df_json.select(\n",
    "    from_json(df_json.json, schema)\n",
    "    .alias('raw_data')) \\\n",
    "  .select('raw_data.data') \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName('exploded_json') \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Available: False'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                data|\n",
      "+--------------------+\n",
      "|    I love this pony|\n",
      "|This restaurant i...|\n",
      "|The weather is ba...|\n",
      "|I will go to the ...|\n",
      "|   She likes to swim|\n",
      "|Apple is a great ...|\n",
      "|    I love this pony|\n",
      "|This restaurant i...|\n",
      "|The weather is ba...|\n",
      "|I will go to the ...|\n",
      "|   She likes to swim|\n",
      "|Apple is a great ...|\n",
      "|    I love this pony|\n",
      "|This restaurant i...|\n",
      "|The weather is ba...|\n",
      "|I will go to the ...|\n",
      "|   She likes to swim|\n",
      "|Apple is a great ...|\n",
      "|    I love this pony|\n",
      "|This restaurant i...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"Data Available: {query.status['isDataAvailable']}\")\n",
    "display(spark.sql('SELECT * FROM exploded_json').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST NLP Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment_analysis(data):\n",
    "    import requests\n",
    "    import json\n",
    "    print(f\"sending request in format: {json.loads(data)}\")\n",
    "    #result = requests.post('http://localhost:9000/predict', json=json.loads(data))\n",
    "    result = requests.post('http://sentiment:9000/predict', json=json.loads(data))\n",
    "    print(json.dumps(result.json()))\n",
    "    return json.dumps(result.json())\n",
    "\n",
    "vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_input = StructType([StructField('data', StringType())])\n",
    "\n",
    "schema_output = StructType(\n",
    "    [StructField('neg', StringType()),\n",
    "     StructField('pos', StringType()),\n",
    "     StructField('neu', StringType()),\n",
    "     StructField('compound', StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f27fbb23e50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_json.select(\n",
    "    from_json(df_json.json, schema)\n",
    "    .alias('raw_data'))\n",
    "  .select('raw_data.data')\n",
    "  .writeStream\n",
    "  .trigger(once=True)\n",
    "  .format(\"memory\")\n",
    "  .queryName('exploded_json')\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f282150cdf0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_json\n",
    "    .select(\n",
    "        from_json(df_json.json, schema_input)\n",
    "        .alias('sentence'),\n",
    "        from_json(vader_udf(df_json.json), schema_output)\n",
    "        .alias('response')\n",
    "    )\n",
    "    .select('sentence.data', 'response.*')\n",
    "    .writeStream\n",
    "    .trigger(once=True)\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"input_output\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAMHERE.\n",
    "Issues connecting to localhost 9000. Going to serve the app on separate named docker instance for serving flask apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Available: False'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+--------+\n",
      "|data|neg|pos|neu|compound|\n",
      "+----+---+---+---+--------+\n",
      "+----+---+---+---+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-249b35bba72d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data Available: {query.status['isDataAvailable']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM input_output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Data Available: {query.status['isDataAvailable']}\")\n",
    "    display(spark.sql('SELECT * FROM input_output').show())\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " StructField('compound', StringType())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    sentence#36.data AS data#45, \n",
    "    response#38.neg AS neg#41, \n",
    "    response#38.pos AS pos#42, \n",
    "    response#38.neu AS neu#43, \n",
    "    response#38.compound AS compound#44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from_json(\n",
    "        StructField(data,StringType,true), json#21, \n",
    "        Some(Etc/UTC)\n",
    "        ) AS sentence#36, \n",
    "    from_json(\n",
    "        StructField(neg,StringType,true), \n",
    "        StructField(pos,StringType,true), \n",
    "        StructField(neu,StringType,true), \n",
    "        StructField(compound,StringType,true), \n",
    "        <lambda>(json#21), \n",
    "        Some(Etc/UTC)) AS response#38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cast(value#8 as string) AS json#21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    StreamingDataSourceV2Relation \n",
    "        [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()\n",
    "\n",
    "query = (\n",
    "    df\n",
    "    .writeStream\n",
    "    .trigger(once=True)\n",
    "    .format(\"memory\")\n",
    "    .queryName('response')\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(query.status)\n",
    "display(spark.sql('SELECT * FROM response').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

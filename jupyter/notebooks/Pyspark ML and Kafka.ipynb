{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook connects to **kafka** to send and receive messages. These messages are then sent to a **Flask App** which serves a restful interaction with a **sentiment analysis** algorithm using python's **NLTK package** and returns a prediction on sentiment given a text document as input. The data is handled using **Spark**. Specifically, PySpark SQL [Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) module which is ran in this jupyter service instance.\n",
    "\n",
    "This is all being ran locally, but can be migrated to a scalable cluster. The scripting in this notebook can be converted into a script *(idealy in scala)* to be launched in production. This notebook allows for prototyping of ml-based real-time solutions utilizing kafka and spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, these queries all end in `.start()` instead of `.awaitTermination()` because we are sending results to memory to allow output to jupyter notebook instead of console. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SCALA_VERSION=\"2.12\"\n",
    "SPARK_VERSION=\"3.0.1\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f\"--packages=org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# To display spark sql streaming output\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Version expected 3.8.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.6\n"
     ]
    }
   ],
   "source": [
    "# Python 3.8.6\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expecting confluent-kafka version 1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: confluent-kafka\n",
      "Version: 1.5.0\n",
      "Summary: Confluent's Python client for Apache Kafka\n",
      "Home-page: https://github.com/confluentinc/confluent-kafka-python\n",
      "Author: Confluent Inc\n",
      "Author-email: support@confluent.io\n",
      "License: UNKNOWN\n",
      "Location: /opt/conda/lib/python3.8/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Version: 1.5.0\n",
    "!pip show confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see what version of kafka is running, navigate to the kafka service in Docker. Open the CLI to access interactive shell. Then run the following commands:\n",
    "    \n",
    "    /bin/bash \n",
    "    find /usr/share/java/kafka -name \\*kafka_\\* | head -1 | grep -o '\\kafka[^\\n]*'\n",
    "We are expecting to see kafka_2.11-2.2.0. This is scala 2.11 and kafka 2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending Messages to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_report(err, msg):    \n",
    "    \"\"\" Called once for each message produced to indicate delivery result.\n",
    "        Triggered by poll() or flush(). \"\"\"\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        print(f'Message delivered to {msg.topic()}')\n",
    "\n",
    "def confluent_kafka_producer(messages, topic, bootstrap_servers):\n",
    "    \n",
    "    p = Producer({'bootstrap.servers': bootstrap_servers})\n",
    "    for data in messages:\n",
    "        \n",
    "        record_key = str(uuid.uuid4())\n",
    "        record_value = json.dumps({'data': data})\n",
    "        p.produce(topic, key=record_key, value=record_value, on_delivery=delivery_report)\n",
    "       \n",
    "        p.poll(0)\n",
    "\n",
    "    p.flush()\n",
    "    print(f\"we've sent {len(messages)} messages to {bootstrap_servers}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Query Output. Used for checking intermediate steps. Needs to print try and display a few times to get output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_results(query, query_name):\n",
    "\n",
    "    for i in range(3):\n",
    "        clear_output(wait=True)\n",
    "        display(f\"{query.status}\")\n",
    "        display(spark.sql(f'SELECT * FROM {query_name}').show())\n",
    "        sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send request for sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment_analysis(data):\n",
    "    \n",
    "    result = requests.post(ML_PREDICT_URL, json=json.loads(data))\n",
    "    return json.dumps(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka\n",
    "BOOTSTRAP_SERVERS = 'kafka:9092'\n",
    "KAFKA_TOPIC = 'test'\n",
    "\n",
    "# Flask App\n",
    "ML_CONTAINER_HOSTNAME=\"ml\"\n",
    "ML_CONTAINER_PORT=\"9000\"\n",
    "ML_PREDICT_URL=f'http://{ML_CONTAINER_HOSTNAME}:{ML_CONTAINER_PORT}/predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize PySpark SQL Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('RealtimeKafkaML') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expecting Spark 3.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.0.1\n"
     ]
    }
   ],
   "source": [
    "# Spark 3.0.1\n",
    "print(f\"Spark {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Messages to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a list of text sentences and send them to a kafka topic labeled `\"test\"`. The kafka docker container is exposing **port 9092** under the **kafka hostname**. This is defined in the docker compose file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_count = 5 # Not sure what this is used for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_messages = [\n",
    "'I love this pony',\n",
    "'This restaurant is great',\n",
    "'The weather is bad today',\n",
    "'I will go to the beach this weekend',\n",
    "'She likes to swim',\n",
    "'Apple is a great company'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confluent_kafka_producer(messages=simple_messages, \n",
    "                         topic=KAFKA_TOPIC, bootstrap_servers=BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read From Kafka to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read From Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', BOOTSTRAP_SERVERS)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option('subscribe', KAFKA_TOPIC)\n",
    "    .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df_raw.selectExpr('CAST(value AS STRING) as json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_name = 'df_json'\n",
    "query = (\n",
    "    df_json \n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(query_name)\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show kafka stream to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_query_results(query, query_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode nested json into table format where each row is text, under the column labeled `data` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('data', StringType())])\n",
    "\n",
    "query_name = 'exploded_json'\n",
    "query = (\n",
    "    df_json.select(\n",
    "        from_json(df_json.json, schema)\n",
    "        .alias('raw_data'))\n",
    "    .select('raw_data.data')\n",
    "    .writeStream\n",
    "    .trigger(once=True)\n",
    "    .format(\"memory\")\n",
    "    .queryName(query_name)\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_query_results(query, query_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST NLP Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Defined function to use with spark to send column data to sentiment analysis endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_input = StructType([StructField('data', StringType())])\n",
    "\n",
    "schema_output = StructType(\n",
    "    [StructField('neg', StringType()),\n",
    "     StructField('pos', StringType()),\n",
    "     StructField('neu', StringType()),\n",
    "     StructField('compound', StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_name = 'input_output'\n",
    "query = (\n",
    "    df_json\n",
    "    .select(\n",
    "        from_json(df_json.json, schema_input)\n",
    "        .alias('sentence'),\n",
    "        from_json(vader_udf(df_json.json), schema_output)\n",
    "        .alias('response')\n",
    "    )\n",
    "    .select('sentence.data', 'response.*')\n",
    "    .writeStream\n",
    "    .trigger(once=True)\n",
    "    .format(\"memory\")\n",
    "    .queryName(query_name)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the Prediction outputs appended to data input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_query_results(query, query_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_twitter_secrets(secrets_path=\"/home/jovyan/.secrets/twitter.json\"):    \n",
    "    with open(secrets_path, 'r') as f:\n",
    "        twitter_secrets = json.load(f)\n",
    "    return twitter_secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_secrets = load_twitter_secrets()\n",
    "# TODO: Put this token in file outside of notebook.\n",
    "CONSUMER_KEY=twitter_secrets.get(\"api_key\")\n",
    "CONSUMER_SECRET=twitter_secrets.get(\"api_secret_key\")\n",
    "CONSUMER_TOKEN=twitter_secrets.get(\"access_token\")\n",
    "CONSUMER_TOKEN_SECRET=twitter_secrets.get(\"access_token_secret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(key=CONSUMER_TOKEN, secret=CONSUMER_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Listener Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_TOPIC=\"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-3036ff6f3759>:10: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(KAFKA_TOPIC, key=record_key, value=record_value)\n",
      "<ipython-input-19-3036ff6f3759>:10: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(KAFKA_TOPIC, key=record_key, value=record_value)\n"
     ]
    }
   ],
   "source": [
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    \n",
    "    def on_data(self, data):\n",
    "        p = Producer({'bootstrap.servers': BOOTSTRAP_SERVERS})\n",
    "        #record_key = str(uuid.uuid4())\n",
    "        record_key = TWITTER_TOPIC\n",
    "        record_value = json.dumps({'data': data})\n",
    "        p.produce(KAFKA_TOPIC, key=record_key, value=record_value)\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "        \n",
    "    def on_error(self, status_code):\n",
    "        print(f\"ERROR: {status_code}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-3036ff6f3759>:10: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(KAFKA_TOPIC, key=record_key, value=record_value)\n"
     ]
    }
   ],
   "source": [
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth=api.auth, listener=myStreamListener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-3036ff6f3759>:10: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(KAFKA_TOPIC, key=record_key, value=record_value)\n"
     ]
    }
   ],
   "source": [
    "myStream.filter(track=[TWITTER_TOPIC], is_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-3036ff6f3759>:10: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(KAFKA_TOPIC, key=record_key, value=record_value)\n"
     ]
    }
   ],
   "source": [
    "df_twitter_raw = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', BOOTSTRAP_SERVERS)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option('subscribe', KAFKA_TOPIC)\n",
    "    .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-3036ff6f3759>:10: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(KAFKA_TOPIC, key=record_key, value=record_value)\n"
     ]
    }
   ],
   "source": [
    "query_name = 'df_twitter_raw3'\n",
    "query = (\n",
    "    df_twitter_raw\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(query_name)\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show kafka stream to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'message': 'Getting offsets from KafkaV2[Subscribe[test]]', 'isDataAvailable': False, 'isTriggerActive': True}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_query_results(query, query_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Real Time ML with Kafka and Spark](https://github.com/BogdanCojocar/medium-articles/tree/master/realtime_kafka)\n",
    "* [For Redis for Flask caching](https://docs.docker.com/compose/gettingstarted/) \n",
    "* [Networking for flask app](https://pythonspeed.com/articles/docker-connection-refused/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://docs.tweepy.org/en/v3.10.0/streaming_how_to.html\n",
    "* https://www.bmc.com/blogs/working-streaming-twitter-data-using-kafka/\n",
    "* https://towardsdatascience.com/using-kafka-to-optimize-data-flow-of-your-twitter-stream-90523d25f3e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
